## ã‚‚ã†æ€§èƒ½ã§è¿·ã‚ãªã„ï¼GCP Cloud TPUãƒ»GPUã‚’TypeScriptã§å®Œå…¨åˆ¶å¾¡ã™ã‚‹æ©Ÿæ¢°å­¦ç¿’åŸºç›¤æ§‹ç¯‰ã‚¬ã‚¤ãƒ‰

## ã¯ã˜ã‚ã«

ã€Œæ©Ÿæ¢°å­¦ç¿’ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãŒé…ã™ãã¦é–‹ç™ºãŒé€²ã¾ãªã„ã€ã€ŒTPUã¨GPUã®ä½¿ã„åˆ†ã‘ãŒã‚ˆãã‚ã‹ã‚‰ãªã„ã€

ãã‚“ãªæ‚©ã¿ã‚’æŠ±ãˆã‚‹æ©Ÿæ¢°å­¦ç¿’ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ã®æ–¹ã«æœ—å ±ã§ã™ã€‚Google Cloud Platform ã® **Cloud TPU** ã¨ **GPU ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹** ãªã‚‰ã€TypeScript ã‚’ä½¿ã£ã¦é«˜æ€§èƒ½ãªæ©Ÿæ¢°å­¦ç¿’åŸºç›¤ã‚’åŠ¹ç‡çš„ã«æ§‹ç¯‰ãƒ»ç®¡ç†ã§ãã¾ã™ã€‚

ã“ã®è¨˜äº‹ã§ã¯ã€**Google Cloud TPUãƒ»GPU** ã®åŸºæœ¬æ¦‚å¿µã‹ã‚‰ã€**TypeScript ã«ã‚ˆã‚‹å®Ÿè£…ä¾‹** ã¾ã§ã€å®Ÿéš›ã®é‹ç”¨ã§ä½¿ãˆã‚‹å®Ÿè·µçš„ãªã‚³ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’è§£èª¬ã—ã¾ã™ã€‚è¨˜äº‹ã‚’èª­ã¿çµ‚ãˆã‚‹é ƒã«ã¯ã€ã‚ãªãŸã‚‚é«˜æ€§èƒ½ãªAIåŸºç›¤ã‚’ãƒ—ãƒ­ã‚°ãƒ©ãƒãƒ–ãƒ«ã«ç®¡ç†ã§ãã‚‹ã‚ˆã†ã«ãªã£ã¦ã„ã‚‹ã¯ãšã§ã™ ğŸš€

>* Cloud TPU ã¨ GPU ã®ç‰¹å¾´ãƒ»ä½¿ã„åˆ†ã‘ã®ç†è§£
>* TypeScript + Google Cloud Client Library ã§ã® TPUãƒ»GPU ç®¡ç†è‡ªå‹•åŒ–
>* æ©Ÿæ¢°å­¦ç¿’ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–ã®å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³
>* ã‚³ã‚¹ãƒˆåŠ¹ç‡ã‚’è€ƒæ…®ã—ãŸãƒªã‚½ãƒ¼ã‚¹ç®¡ç†æˆ¦ç•¥

## Cloud TPU ã¨ GPUã¨ã¯

Google Cloud Platform ã§ã¯ã€æ©Ÿæ¢°å­¦ç¿’ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã«å¿œã˜ã¦æœ€é©åŒ–ã•ã‚ŒãŸ2ã¤ã®é¸æŠè‚¢ã‚’æä¾›ã—ã¦ã„ã¾ã™ã€‚

### 1\. Cloud TPUã®ç‰¹å¾´

**Cloud TPUï¼ˆTensor Processing Unitï¼‰** ã¯ã€GoogleãŒæ©Ÿæ¢°å­¦ç¿’å°‚ç”¨ã«è¨­è¨ˆã—ãŸã‚«ã‚¹ã‚¿ãƒ ASICã§ã™ã€‚

**ğŸ’¡ TPUãŒæœ€é©ãªç”¨é€”**

>* å¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼ˆLLMï¼‰ã®å­¦ç¿’ãƒ»æ¨è«–
>* å¤§é‡ã®è¡Œåˆ—è¨ˆç®—ã‚’å«ã‚€æ·±å±¤å­¦ç¿’ãƒ¢ãƒ‡ãƒ«
>* 128x128ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã§ãã‚‹å¯†åº¦ã®é«˜ã„è¨ˆç®—å‡¦ç†
>* TensorFlowã‚„PyTorchã§ã®æ¨™æº–çš„ãªæ¼”ç®—

**ğŸ“Š æœ€æ–°ã®TPUæ€§èƒ½ï¼ˆ2025å¹´ç‰ˆï¼‰**

| TPUãƒãƒ¼ã‚¸ãƒ§ãƒ³ | ç™ºè¡¨å¹´ | æ€§èƒ½å‘ä¸Š | ç‰¹å¾´ |
|-------------|-------|----------|------|
| **TPU v6** | 2024å¹´5æœˆ | v5eã®4.7å€ | 256x256 MXUæ­è¼‰ |
| **TPU v7 (Ironwood)** | 2025å¹´4æœˆ | å¤§å¹…æ€§èƒ½å‘ä¸Š | 256ãƒãƒƒãƒ—ãƒ»9,216ãƒãƒƒãƒ—æ§‹æˆ |
| **TPU v6e** | 2024å¹´ | é«˜ã‚³ã‚¹ãƒ‘ | 256 x 256 multiply-accumulators |

### 2\. GPU ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®ç‰¹å¾´

**GPU ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹** ã¯ã€NVIDIAè£½ã®æ±ç”¨GPU ã‚’æ­è¼‰ã—ãŸä»®æƒ³ãƒã‚·ãƒ³ã§ã™ã€‚

**ğŸ’¡ GPUãŒæœ€é©ãªç”¨é€”**

>* è¿…é€Ÿãªãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ”ãƒ³ã‚°ã¨æœ€å¤§é™ã®æŸ”è»Ÿæ€§ãŒå¿…è¦ãªå ´åˆ
>* ã‚«ã‚¹ã‚¿ãƒ  PyTorch/JAX ã‚ªãƒšãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«
>* Cloud TPU ã§åˆ©ç”¨ã§ããªã„ TensorFlow æ¼”ç®—å­ã‚’ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«
>* ä¸¦åˆ—å‡¦ç†ãŒå¯èƒ½ãªå¤šæ§˜ãªè¨ˆç®—ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰

**ğŸ“Š æœ€æ–°ã®GPUé¸æŠè‚¢ï¼ˆ2025å¹´ç‰ˆï¼‰**

| ãƒã‚·ãƒ³ã‚¿ã‚¤ãƒ— | GPU | ãƒ¡ãƒ¢ãƒª | é©ç”¨å ´é¢ |
|-------------|-----|--------|---------|
| **A4X** | NVIDIA GB200 Grace Blackwell | è¶…å¤§å®¹é‡ | åŸºç›¤ãƒ¢ãƒ‡ãƒ«ã®å­¦ç¿’ãƒ»æ¨è«– |
| **A3 Ultra** | NVIDIA H200 SXM | 141GB HBM3e | æœ€é«˜æ€§èƒ½MLå‡¦ç† |
| **A3** | NVIDIA H100 SXM | 80GB HBM3 | å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ |
| **G2** | NVIDIA L4 | 24GB GDDR6 | æ¨è«–ãƒ»è»½é‡å­¦ç¿’ |

**âš–ï¸ TPU vs GPUï¼šä½¿ã„åˆ†ã‘ã®åˆ¤æ–­åŸºæº–**

| æ¯”è¼ƒé …ç›® | Cloud TPU | GPU ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ |
|---------|-----------|----------------|
| **è¨ˆç®—ç‰¹æ€§** | è¡Œåˆ—è¨ˆç®—ç‰¹åŒ– | æ±ç”¨ä¸¦åˆ—å‡¦ç† |
| **é–‹ç™ºæŸ”è»Ÿæ€§** | åˆ¶ç´„ã‚ã‚Š | é«˜ã„æŸ”è»Ÿæ€§ |
| **ã‚³ã‚¹ãƒˆåŠ¹ç‡** | å¤§è¦æ¨¡å‡¦ç†ã§å„ªç§€ | ç”¨é€”ã«ã‚ˆã‚Šå¤‰å‹• |
| **ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—** | ã‚·ãƒ³ãƒ—ãƒ« | è¤‡é›‘ãªè¨­å®šãŒå¿…è¦ |
| **ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯** | TensorFlow/PyTorch | å¤šæ•°ã‚µãƒãƒ¼ãƒˆ |

## é–‹ç™ºç’°å¢ƒã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

Cloud TPU ã¨ GPU ã‚’TypeScriptã§ç®¡ç†ã™ã‚‹ãŸã‚ã®é–‹ç™ºç’°å¢ƒã‚’æ§‹ç¯‰ã—ã¾ã—ã‚‡ã†ã€‚

**ğŸ“¦ å¿…è¦ãªãƒ„ãƒ¼ãƒ«ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«**

```bash
# Google Cloud SDK ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
# https://cloud.google.com/sdk/docs/install ã‹ã‚‰ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

# Cloud SDK ã®åˆæœŸåŒ–ã¨ãƒ­ã‚°ã‚¤ãƒ³
gcloud init
gcloud auth login
gcloud auth application-default login

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ä½œæˆã¨è¨­å®š
gcloud projects create your-ml-project --name="ML Platform Project"
gcloud config set project your-ml-project

# å¿…è¦ãªAPIã®æœ‰åŠ¹åŒ–
gcloud services enable compute.googleapis.com
gcloud services enable tpu.googleapis.com
gcloud services enable container.googleapis.com
```

**ğŸš€ TypeScriptãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®åˆæœŸåŒ–**

```bash
# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
mkdir gcp-ml-platform && cd gcp-ml-platform

# Node.jsãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®åˆæœŸåŒ–
npm init -y

# Google Cloud Client Library ã®ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«
npm install @google-cloud/tpu @google-cloud/compute
npm install -D typescript @types/node ts-node nodemon

# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ãƒ©ã‚¤ãƒ–ãƒ©ãƒª
npm install dotenv winston googleapis
npm install -D @types/dotenv
```

**ğŸ“ TypeScriptè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ**

```json:tsconfig.json
{
  "compilerOptions": {
    "target": "ES2020",
    "lib": ["ES2020"],
    "module": "commonjs",
    "moduleResolution": "node",
    "rootDir": "./src",
    "outDir": "./dist",
    "strict": true,
    "esModuleInterop": true,
    "skipLibCheck": true,
    "forceConsistentCasingInFileNames": true,
    "resolveJsonModule": true,
    "declaration": true,
    "sourceMap": true,
    "removeComments": true
  },
  "include": ["src/**/*"],
  "exclude": ["node_modules", "dist"]
}
```

## Cloud TPUç®¡ç†ã®å®Ÿè£…

### 1\. TPU ãƒãƒ¼ãƒ‰ç®¡ç†ã‚¯ãƒ©ã‚¹

```typescript:src/services/TPUManager.ts
import { TpuClient } from '@google-cloud/tpu';
import { logger } from '../utils/logger';

export interface TPUConfig {
  name: string;
  zone: string;
  acceleratorType: string;
  runtimeVersion: string;
  networkConfig?: {
    network?: string;
    subnetwork?: string;
  };
  labels?: Record<string, string>;
}

export interface TPUInfo {
  name: string;
  state: string;
  acceleratorType: string;
  runtimeVersion: string;
  networkEndpoints?: Array<{
    ipAddress: string;
    port: number;
  }>;
  createTime: string;
}

export class TPUManager {
  private tpuClient: TpuClient;
  private projectId: string;

  constructor(projectId: string) {
    this.projectId = projectId;
    this.tpuClient = new TpuClient();
  }

  async createTPUNode(config: TPUConfig): Promise<TPUInfo> {
    const {
      name,
      zone,
      acceleratorType,
      runtimeVersion,
      networkConfig = {},
      labels = {}
    } = config;

    const parent = `projects/${this.projectId}/locations/${zone}`;
    
    const nodeResource = {
      acceleratorType,
      runtimeVersion,
      networkConfig: {
        network: networkConfig.network || 'default',
        subnetwork: networkConfig.subnetwork || 'default',
      },
      labels: {
        ...labels,
        'created-by': 'typescript-ml-platform',
        'environment': process.env.NODE_ENV || 'development'
      },
    };

    try {
      logger.info(`Creating TPU node: ${name} in zone: ${zone}`);
      
      const [operation] = await this.tpuClient.createNode({
        parent,
        nodeId: name,
        node: nodeResource,
      });

      // ã‚ªãƒšãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†ã‚’å¾…ã¤
      const [response] = await operation.promise();
      
      logger.info(`Successfully created TPU node: ${name}`);
      return this.formatTPUInfo(response);

    } catch (error) {
      logger.error(`Failed to create TPU node ${name}:`, error);
      throw new Error(`TPU node creation failed: ${error.message}`);
    }
  }

  async getTPUNode(name: string, zone: string): Promise<TPUInfo> {
    try {
      const nodeName = `projects/${this.projectId}/locations/${zone}/nodes/${name}`;
      
      const [node] = await this.tpuClient.getNode({
        name: nodeName,
      });

      return this.formatTPUInfo(node);

    } catch (error) {
      logger.error(`Failed to get TPU node ${name}:`, error);
      throw new Error(`TPU node retrieval failed: ${error.message}`);
    }
  }

  async listTPUNodes(zone: string): Promise<TPUInfo[]> {
    try {
      const parent = `projects/${this.projectId}/locations/${zone}`;
      
      const [nodes] = await this.tpuClient.listNodes({
        parent,
      });

      return nodes.map(node => this.formatTPUInfo(node));

    } catch (error) {
      logger.error(`Failed to list TPU nodes in zone ${zone}:`, error);
      throw new Error(`TPU node listing failed: ${error.message}`);
    }
  }

  async deleteTPUNode(name: string, zone: string): Promise<void> {
    try {
      logger.info(`Deleting TPU node: ${name} in zone: ${zone}`);
      
      const nodeName = `projects/${this.projectId}/locations/${zone}/nodes/${name}`;
      
      const [operation] = await this.tpuClient.deleteNode({
        name: nodeName,
      });

      await operation.promise();
      
      logger.info(`Successfully deleted TPU node: ${name}`);

    } catch (error) {
      logger.error(`Failed to delete TPU node ${name}:`, error);
      throw new Error(`TPU node deletion failed: ${error.message}`);
    }
  }

  async startTPUNode(name: string, zone: string): Promise<void> {
    try {
      logger.info(`Starting TPU node: ${name}`);
      
      const nodeName = `projects/${this.projectId}/locations/${zone}/nodes/${name}`;
      
      const [operation] = await this.tpuClient.startNode({
        name: nodeName,
      });

      await operation.promise();
      
      logger.info(`Successfully started TPU node: ${name}`);

    } catch (error) {
      logger.error(`Failed to start TPU node ${name}:`, error);
      throw new Error(`TPU node start failed: ${error.message}`);
    }
  }

  async stopTPUNode(name: string, zone: string): Promise<void> {
    try {
      logger.info(`Stopping TPU node: ${name}`);
      
      const nodeName = `projects/${this.projectId}/locations/${zone}/nodes/${name}`;
      
      const [operation] = await this.tpuClient.stopNode({
        name: nodeName,
      });

      await operation.promise();
      
      logger.info(`Successfully stopped TPU node: ${name}`);

    } catch (error) {
      logger.error(`Failed to stop TPU node ${name}:`, error);
      throw new Error(`TPU node stop failed: ${error.message}`);
    }
  }

  async getAvailableAcceleratorTypes(zone: string): Promise<string[]> {
    try {
      const parent = `projects/${this.projectId}/locations/${zone}`;
      
      const [acceleratorTypes] = await this.tpuClient.listAcceleratorTypes({
        parent,
      });

      return acceleratorTypes.map(type => type.name!.split('/').pop()!);

    } catch (error) {
      logger.error(`Failed to get accelerator types for zone ${zone}:`, error);
      throw new Error(`Accelerator types retrieval failed: ${error.message}`);
    }
  }

  async getAvailableRuntimeVersions(zone: string): Promise<string[]> {
    try {
      const parent = `projects/${this.projectId}/locations/${zone}`;
      
      const [runtimeVersions] = await this.tpuClient.listRuntimeVersions({
        parent,
      });

      return runtimeVersions.map(version => version.version!);

    } catch (error) {
      logger.error(`Failed to get runtime versions for zone ${zone}:`, error);
      throw new Error(`Runtime versions retrieval failed: ${error.message}`);
    }
  }

  private formatTPUInfo(node: any): TPUInfo {
    return {
      name: node.name!.split('/').pop()!,
      state: node.state!,
      acceleratorType: node.acceleratorType!,
      runtimeVersion: node.runtimeVersion!,
      networkEndpoints: node.networkEndpoints?.map((endpoint: any) => ({
        ipAddress: endpoint.ipAddress,
        port: endpoint.port || 8470,
      })),
      createTime: node.createTime!,
    };
  }
}
```

### 2\. GPU ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç®¡ç†ã®å®Ÿè£…

```typescript:src/services/GPUManager.ts
import { InstancesClient, ZoneOperationsClient } from '@google-cloud/compute';
import { logger } from '../utils/logger';

export interface GPUConfig {
  name: string;
  zone: string;
  machineType: string;
  gpuType: string;
  gpuCount: number;
  diskImage: string;
  diskSize?: number;
  tags?: string[];
  labels?: Record<string, string>;
  metadata?: Record<string, string>;
  preemptible?: boolean;
}

export interface GPUInstanceInfo {
  name: string;
  status: string;
  machineType: string;
  gpuInfo: {
    type: string;
    count: number;
  };
  zone: string;
  internalIP?: string;
  externalIP?: string;
  preemptible: boolean;
  createdAt: string;
}

export class GPUManager {
  private instancesClient: InstancesClient;
  private operationsClient: ZoneOperationsClient;
  private projectId: string;

  constructor(projectId: string) {
    this.projectId = projectId;
    this.instancesClient = new InstancesClient();
    this.operationsClient = new ZoneOperationsClient();
  }

  async createGPUInstance(config: GPUConfig): Promise<GPUInstanceInfo> {
    const {
      name,
      zone,
      machineType,
      gpuType,
      gpuCount,
      diskImage,
      diskSize = 50,
      tags = [],
      labels = {},
      metadata = {},
      preemptible = false
    } = config;

    const instanceResource = {
      name,
      machineType: `zones/${zone}/machineTypes/${machineType}`,
      disks: [
        {
          boot: true,
          autoDelete: true,
          initializeParams: {
            sourceImage: `projects/ml-images/global/images/${diskImage}`,
            diskSizeGb: diskSize.toString(),
            diskType: `zones/${zone}/diskTypes/pd-ssd`,
          },
        },
      ],
      networkInterfaces: [
        {
          network: 'global/networks/default',
          accessConfigs: [
            {
              type: 'ONE_TO_ONE_NAT',
              name: 'External NAT',
            },
          ],
        },
      ],
      guestAccelerators: [
        {
          acceleratorType: `zones/${zone}/acceleratorTypes/${gpuType}`,
          acceleratorCount: gpuCount,
        },
      ],
      scheduling: {
        onHostMaintenance: 'TERMINATE',
        preemptible,
      },
      tags: {
        items: ['gpu-instance', 'ml-workload', ...tags],
      },
      labels: {
        ...labels,
        'instance-type': 'gpu',
        'created-by': 'typescript-ml-platform',
      },
      metadata: {
        items: [
          {
            key: 'startup-script',
            value: this.getGPUStartupScript(),
          },
          ...Object.entries(metadata).map(([key, value]) => ({
            key,
            value,
          })),
        ],
      },
    };

    try {
      logger.info(`Creating GPU instance: ${name} with ${gpuCount}x ${gpuType}`);
      
      const [operation] = await this.instancesClient.insert({
        project: this.projectId,
        zone,
        instanceResource,
      });

      // ã‚ªãƒšãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†ã‚’å¾…ã¤
      await this.waitForOperation(operation.name!, zone);
      
      // ä½œæˆã•ã‚ŒãŸã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®è©³ç´°ã‚’å–å¾—
      const instanceInfo = await this.getGPUInstance(name, zone);
      
      logger.info(`Successfully created GPU instance: ${name}`);
      return instanceInfo;

    } catch (error) {
      logger.error(`Failed to create GPU instance ${name}:`, error);
      throw new Error(`GPU instance creation failed: ${error.message}`);
    }
  }

  async getGPUInstance(name: string, zone: string): Promise<GPUInstanceInfo> {
    try {
      const [instance] = await this.instancesClient.get({
        project: this.projectId,
        zone,
        instance: name,
      });

      const networkInterface = instance.networkInterfaces?.[0];
      const accessConfig = networkInterface?.accessConfigs?.[0];
      const gpu = instance.guestAccelerators?.[0];

      return {
        name: instance.name!,
        status: instance.status!,
        machineType: instance.machineType!.split('/').pop()!,
        gpuInfo: {
          type: gpu?.acceleratorType?.split('/').pop()! || 'none',
          count: gpu?.acceleratorCount || 0,
        },
        zone,
        internalIP: networkInterface?.networkIP,
        externalIP: accessConfig?.natIP,
        preemptible: instance.scheduling?.preemptible || false,
        createdAt: instance.creationTimestamp!,
      };

    } catch (error) {
      logger.error(`Failed to get GPU instance ${name}:`, error);
      throw new Error(`GPU instance retrieval failed: ${error.message}`);
    }
  }

  async listGPUInstances(zone: string): Promise<GPUInstanceInfo[]> {
    try {
      const [instances] = await this.instancesClient.list({
        project: this.projectId,
        zone,
        filter: 'labels.instance-type=gpu',
      });

      return instances.map(instance => {
        const networkInterface = instance.networkInterfaces?.[0];
        const accessConfig = networkInterface?.accessConfigs?.[0];
        const gpu = instance.guestAccelerators?.[0];

        return {
          name: instance.name!,
          status: instance.status!,
          machineType: instance.machineType!.split('/').pop()!,
          gpuInfo: {
            type: gpu?.acceleratorType?.split('/').pop()! || 'none',
            count: gpu?.acceleratorCount || 0,
          },
          zone,
          internalIP: networkInterface?.networkIP,
          externalIP: accessConfig?.natIP,
          preemptible: instance.scheduling?.preemptible || false,
          createdAt: instance.creationTimestamp!,
        };
      });

    } catch (error) {
      logger.error(`Failed to list GPU instances in zone ${zone}:`, error);
      throw new Error(`GPU instance listing failed: ${error.message}`);
    }
  }

  async deleteGPUInstance(name: string, zone: string): Promise<void> {
    try {
      logger.info(`Deleting GPU instance: ${name} in zone: ${zone}`);
      
      const [operation] = await this.instancesClient.delete({
        project: this.projectId,
        zone,
        instance: name,
      });

      await this.waitForOperation(operation.name!, zone);
      
      logger.info(`Successfully deleted GPU instance: ${name}`);

    } catch (error) {
      logger.error(`Failed to delete GPU instance ${name}:`, error);
      throw new Error(`GPU instance deletion failed: ${error.message}`);
    }
  }

  private getGPUStartupScript(): string {
    return `#!/bin/bash
    # NVIDIA Driver and CUDA installation
    curl -O https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.0-1_all.deb
    sudo dpkg -i cuda-keyring_1.0-1_all.deb
    sudo apt-get update
    sudo apt-get -y install cuda
    
    # Docker installation for containerized ML workloads
    curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
    echo "deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null
    sudo apt-get update
    sudo apt-get -y install docker-ce docker-ce-cli containerd.io
    
    # NVIDIA Container Toolkit
    curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -
    curl -s -L https://nvidia.github.io/nvidia-docker/ubuntu20.04/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list
    sudo apt-get update && sudo apt-get -y install nvidia-docker2
    sudo systemctl restart docker
    
    # Create ML workspace
    sudo mkdir -p /opt/ml-workspace
    sudo chmod 777 /opt/ml-workspace
    
    echo "GPU instance setup completed" | sudo tee /var/log/startup-script.log
    `;
  }

  private async waitForOperation(operationName: string, zone: string): Promise<void> {
    let operation;
    do {
      [operation] = await this.operationsClient.get({
        project: this.projectId,
        zone,
        operation: operationName,
      });

      if (operation.status === 'DONE') {
        if (operation.error) {
          throw new Error(`Operation failed: ${JSON.stringify(operation.error)}`);
        }
        break;
      }

      // 3ç§’é–“å¾…ã£ã¦ã‹ã‚‰å†ãƒã‚§ãƒƒã‚¯
      await new Promise(resolve => setTimeout(resolve, 3000));
    } while (operation.status !== 'DONE');
  }
}
```

##  æ©Ÿæ¢°å­¦ç¿’ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ç®¡ç†

**çµ±åˆãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ã‚¯ãƒ©ã‚¹**

```typescript:src/services/MLPlatformManager.ts
import { TPUManager, TPUConfig } from './TPUManager';
import { GPUManager, GPUConfig } from './GPUManager';
import { logger } from '../utils/logger';

export interface MLWorkloadConfig {
  workloadType: 'training' | 'inference' | 'research';
  model: {
    name: string;
    framework: 'tensorflow' | 'pytorch' | 'jax';
    size: 'small' | 'medium' | 'large' | 'xlarge';
  };
  compute: {
    preferredType: 'tpu' | 'gpu' | 'auto';
    zone: string;
    budget?: number;
  };
  storage: {
    datasetSize?: number;
    modelOutputSize?: number;
  };
}

export interface ComputeRecommendation {
  recommendedType: 'tpu' | 'gpu';
  reasoning: string[];
  estimatedCost: number;
  config: TPUConfig | GPUConfig;
}

export class MLPlatformManager {
  private tpuManager: TPUManager;
  private gpuManager: GPUManager;
  private projectId: string;

  constructor(projectId: string) {
    this.projectId = projectId;
    this.tpuManager = new TPUManager(projectId);
    this.gpuManager = new GPUManager(projectId);
  }

  async recommendCompute(workloadConfig: MLWorkloadConfig): Promise<ComputeRecommendation> {
    const { workloadType, model, compute, storage } = workloadConfig;
    const reasoning: string[] = [];
    let recommendedType: 'tpu' | 'gpu';
    let estimatedCost = 0;

    // ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã‚¿ã‚¤ãƒ—ã«ã‚ˆã‚‹åˆæœŸåˆ¤å®š
    if (compute.preferredType !== 'auto') {
      recommendedType = compute.preferredType;
      reasoning.push(`ãƒ¦ãƒ¼ã‚¶ãƒ¼æŒ‡å®šã«ã‚ˆã‚Š${compute.preferredType.toUpperCase()}ã‚’é¸æŠ`);
    } else {
      // è‡ªå‹•åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯
      if (model.framework === 'tensorflow' && model.size === 'xlarge') {
        recommendedType = 'tpu';
        reasoning.push('å¤§è¦æ¨¡TensorFlowãƒ¢ãƒ‡ãƒ«ã®ãŸã‚TPUãŒæœ€é©');
      } else if (workloadType === 'research' && model.framework === 'pytorch') {
        recommendedType = 'gpu';
        reasoning.push('ç ”ç©¶ç”¨é€”ã§PyTorchã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚GPUãŒé©ã—ã¦ã„ã‚‹');
      } else if (model.size === 'large' || model.size === 'xlarge') {
        recommendedType = 'tpu';
        reasoning.push('å¤§è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã®ãŸã‚TPUãŒã‚³ã‚¹ãƒˆåŠ¹ç‡çš„');
      } else {
        recommendedType = 'gpu';
        reasoning.push('ä¸­å°è¦æ¨¡ãƒ¢ãƒ‡ãƒ«ã§GPUãŒæŸ”è»Ÿæ€§ã¨ã‚³ã‚¹ãƒˆã®ãƒãƒ©ãƒ³ã‚¹ãŒè‰¯ã„');
      }
    }

    // è¨­å®šã¨ã‚³ã‚¹ãƒˆè¦‹ç©ã‚‚ã‚Šã®ç”Ÿæˆ
    let config: TPUConfig | GPUConfig;
    
    if (recommendedType === 'tpu') {
      const acceleratorType = this.getRecommendedTPUType(model.size);
      config = {
        name: `${model.name}-tpu-${Date.now()}`,
        zone: compute.zone,
        acceleratorType,
        runtimeVersion: model.framework === 'tensorflow' ? 'tpu-vm-tf-2.15.0-v4' : 'tpu-vm-pt-2.1-v4',
        labels: {
          workload: workloadType,
          model: model.name,
          framework: model.framework,
        },
      };
      estimatedCost = this.estimateTPUCost(acceleratorType, workloadType);
    } else {
      const { machineType, gpuType, gpuCount } = this.getRecommendedGPUConfig(model.size);
      config = {
        name: `${model.name}-gpu-${Date.now()}`,
        zone: compute.zone,
        machineType,
        gpuType,
        gpuCount,
        diskImage: 'c0-deeplearning-common-cu121-v20240922-ubuntu-2204',
        diskSize: storage?.datasetSize ? Math.max(50, storage.datasetSize * 2) : 100,
        preemptible: workloadType !== 'inference',
        labels: {
          workload: workloadType,
          model: model.name,
          framework: model.framework,
        },
        metadata: {
          'ml-framework': model.framework,
          'model-size': model.size,
        },
      };
      estimatedCost = this.estimateGPUCost(machineType, gpuType, gpuCount, workloadType);
    }

    // äºˆç®—ãƒã‚§ãƒƒã‚¯
    if (compute.budget && estimatedCost > compute.budget) {
      reasoning.push(`âš ï¸ æ¨å®šã‚³ã‚¹ãƒˆ${estimatedCost}å††ãŒäºˆç®—${compute.budget}å††ã‚’è¶…é`);
      // äºˆç®—å†…ã®ä»£æ›¿æ¡ˆã‚’ææ¡ˆ
      if (recommendedType === 'tpu') {
        recommendedType = 'gpu';
        reasoning.push('äºˆç®—ã®åˆ¶ç´„ã«ã‚ˆã‚ŠGPUã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ææ¡ˆ');
        // GPUã®å†è¨ˆç®—...
      }
    }

    return {
      recommendedType,
      reasoning,
      estimatedCost,
      config,
    };
  }

  async deployWorkload(recommendation: ComputeRecommendation): Promise<string> {
    const { recommendedType, config } = recommendation;

    try {
      if (recommendedType === 'tpu') {
        const tpuInfo = await this.tpuManager.createTPUNode(config as TPUConfig);
        logger.info(`TPU workload deployed: ${tpuInfo.name}`);
        return tpuInfo.name;
      } else {
        const gpuInfo = await this.gpuManager.createGPUInstance(config as GPUConfig);
        logger.info(`GPU workload deployed: ${gpuInfo.name}`);
        return gpuInfo.name;
      }
    } catch (error) {
      logger.error('Failed to deploy ML workload:', error);
      throw new Error(`Workload deployment failed: ${error.message}`);
    }
  }

  async getWorkloadStatus(workloadName: string, zone: string, computeType: 'tpu' | 'gpu') {
    try {
      if (computeType === 'tpu') {
        return await this.tpuManager.getTPUNode(workloadName, zone);
      } else {
        return await this.gpuManager.getGPUInstance(workloadName, zone);
      }
    } catch (error) {
      logger.error(`Failed to get workload status for ${workloadName}:`, error);
      throw new Error(`Status retrieval failed: ${error.message}`);
    }
  }

  async cleanupWorkload(workloadName: string, zone: string, computeType: 'tpu' | 'gpu'): Promise<void> {
    try {
      if (computeType === 'tpu') {
        await this.tpuManager.deleteTPUNode(workloadName, zone);
      } else {
        await this.gpuManager.deleteGPUInstance(workloadName, zone);
      }
      logger.info(`Successfully cleaned up workload: ${workloadName}`);
    } catch (error) {
      logger.error(`Failed to cleanup workload ${workloadName}:`, error);
      throw new Error(`Workload cleanup failed: ${error.message}`);
    }
  }

  private getRecommendedTPUType(modelSize: string): string {
    switch (modelSize) {
      case 'small':
        return 'v2-8';
      case 'medium':
        return 'v3-8';
      case 'large':
        return 'v4-8';
      case 'xlarge':
        return 'v5p-8';
      default:
        return 'v3-8';
    }
  }

  private getRecommendedGPUConfig(modelSize: string): {
    machineType: string;
    gpuType: string;
    gpuCount: number;
  } {
    switch (modelSize) {
      case 'small':
        return {
          machineType: 'g2-standard-4',
          gpuType: 'nvidia-l4',
          gpuCount: 1,
        };
      case 'medium':
        return {
          machineType: 'a2-highgpu-1g',
          gpuType: 'nvidia-tesla-a100',
          gpuCount: 1,
        };
      case 'large':
        return {
          machineType: 'a2-highgpu-2g',
          gpuType: 'nvidia-tesla-a100',
          gpuCount: 2,
        };
      case 'xlarge':
        return {
          machineType: 'a3-highgpu-8g',
          gpuType: 'nvidia-h100-80gb',
          gpuCount: 8,
        };
      default:
        return {
          machineType: 'g2-standard-4',
          gpuType: 'nvidia-l4',
          gpuCount: 1,
        };
    }
  }

  private estimateTPUCost(acceleratorType: string, workloadType: string): number {
    // ç°¡ç•¥åŒ–ã•ã‚ŒãŸã‚³ã‚¹ãƒˆè¨ˆç®—ï¼ˆå®Ÿéš›ã®æ–™é‡‘ã¯å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‚ç…§ï¼‰
    const baseHourlyCost = {
      'v2-8': 4.5,
      'v3-8': 8.0,
      'v4-8': 16.0,
      'v5p-8': 24.0,
    }[acceleratorType] || 8.0;

    const estimatedHours = workloadType === 'training' ? 24 : 8;
    return Math.round(baseHourlyCost * estimatedHours);
  }

  private estimateGPUCost(machineType: string, gpuType: string, gpuCount: number, workloadType: string): number {
    // ç°¡ç•¥åŒ–ã•ã‚ŒãŸã‚³ã‚¹ãƒˆè¨ˆç®—ï¼ˆå®Ÿéš›ã®æ–™é‡‘ã¯å…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‚ç…§ï¼‰
    const baseHourlyCost = {
      'nvidia-l4': 0.6,
      'nvidia-tesla-a100': 2.9,
      'nvidia-h100-80gb': 8.5,
    }[gpuType] || 2.0;

    const estimatedHours = workloadType === 'training' ? 24 : 8;
    const preemptibleDiscount = workloadType !== 'inference' ? 0.7 : 1.0;
    
    return Math.round(baseHourlyCost * gpuCount * estimatedHours * preemptibleDiscount);
  }
}
```

## å®Ÿè·µçš„ãªä½¿ç”¨ä¾‹

```typescript:src/examples/mlWorkflowExample.ts
import { MLPlatformManager, MLWorkloadConfig } from '../services/MLPlatformManager';
import { logger } from '../utils/logger';

async function trainLargeLanguageModel() {
  const projectId = process.env.GOOGLE_CLOUD_PROJECT!;
  const mlPlatform = new MLPlatformManager(projectId);

  const workloadConfig: MLWorkloadConfig = {
    workloadType: 'training',
    model: {
      name: 'llama-7b-custom',
      framework: 'pytorch',
      size: 'large',
    },
    compute: {
      preferredType: 'auto',
      zone: 'us-central1-a',
      budget: 5000, // æœˆé¡äºˆç®—ï¼ˆå††ï¼‰
    },
    storage: {
      datasetSize: 50, // GB
      modelOutputSize: 15, // GB
    },
  };

  try {
    // 1. æœ€é©ãªã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãƒªã‚½ãƒ¼ã‚¹ã®æ¨å¥¨ã‚’å–å¾—
    const recommendation = await mlPlatform.recommendCompute(workloadConfig);
    
    logger.info('ğŸ’¡ ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°æ¨å¥¨çµæœ:');
    logger.info(`   æ¨å¥¨ã‚¿ã‚¤ãƒ—: ${recommendation.recommendedType.toUpperCase()}`);
    logger.info(`   æ¨å®šã‚³ã‚¹ãƒˆ: Â¥${recommendation.estimatedCost}`);
    logger.info(`   ç†ç”±:`);
    recommendation.reasoning.forEach(reason => {
      logger.info(`     - ${reason}`);
    });

    // 2. ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã®ãƒ‡ãƒ—ãƒ­ã‚¤
    const workloadName = await mlPlatform.deployWorkload(recommendation);
    logger.info(`ğŸš€ ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã€Œ${workloadName}ã€ã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ã—ã¾ã—ãŸ`);

    // 3. ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã®ç›£è¦–
    const status = await mlPlatform.getWorkloadStatus(
      workloadName,
      workloadConfig.compute.zone,
      recommendation.recommendedType
    );
    logger.info(`ğŸ“Š ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹:`, status);

    // 4. å­¦ç¿’å®Œäº†å¾Œã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ï¼ˆä¾‹ï¼š8æ™‚é–“å¾Œï¼‰
    setTimeout(async () => {
      await mlPlatform.cleanupWorkload(
        workloadName,
        workloadConfig.compute.zone,
        recommendation.recommendedType
      );
      logger.info(`âœ… ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã€Œ${workloadName}ã€ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã¾ã—ãŸ`);
    }, 8 * 60 * 60 * 1000); // 8æ™‚é–“

  } catch (error) {
    logger.error('ML workload failed:', error);
  }
}

async function runInferenceService() {
  const projectId = process.env.GOOGLE_CLOUD_PROJECT!;
  const mlPlatform = new MLPlatformManager(projectId);

  const inferenceConfig: MLWorkloadConfig = {
    workloadType: 'inference',
    model: {
      name: 'bert-qa-service',
      framework: 'tensorflow',
      size: 'medium',
    },
    compute: {
      preferredType: 'gpu', // æ¨è«–ã‚µãƒ¼ãƒ“ã‚¹ã¯GPUã‚’æŒ‡å®š
      zone: 'asia-northeast1-a',
    },
  };

  try {
    const recommendation = await mlPlatform.recommendCompute(inferenceConfig);
    const workloadName = await mlPlatform.deployWorkload(recommendation);
    
    logger.info(`ğŸ” æ¨è«–ã‚µãƒ¼ãƒ“ã‚¹ã€Œ${workloadName}ã€ãŒç¨¼åƒä¸­ã§ã™`);
    
    // æ¨è«–ã‚µãƒ¼ãƒ“ã‚¹ã¯å¸¸é§ã™ã‚‹ãŸã‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã¯æ‰‹å‹•ã¾ãŸã¯åˆ¥ã®ä»•çµ„ã¿ã§

  } catch (error) {
    logger.error('Inference service deployment failed:', error);
  }
}

async function researchExperiment() {
  const projectId = process.env.GOOGLE_CLOUD_PROJECT!;
  const mlPlatform = new MLPlatformManager(projectId);

  const experiments = [
    {
      name: 'vision-transformer-exp1',
      framework: 'pytorch' as const,
      size: 'medium' as const,
    },
    {
      name: 'transformer-ablation-exp2',
      framework: 'pytorch' as const,
      size: 'small' as const,
    },
  ];

  // è¤‡æ•°å®Ÿé¨“ã‚’ä¸¦åˆ—å®Ÿè¡Œ
  const deploymentPromises = experiments.map(async (experiment) => {
    const config: MLWorkloadConfig = {
      workloadType: 'research',
      model: experiment,
      compute: {
        preferredType: 'auto',
        zone: 'us-west1-b',
      },
    };

    try {
      const recommendation = await mlPlatform.recommendCompute(config);
      const workloadName = await mlPlatform.deployWorkload(recommendation);
      
      logger.info(`ğŸ§ª å®Ÿé¨“ã€Œ${workloadName}ã€ã‚’é–‹å§‹ã—ã¾ã—ãŸ`);
      return { workloadName, computeType: recommendation.recommendedType };
      
    } catch (error) {
      logger.error(`å®Ÿé¨“ ${experiment.name} ã®é–‹å§‹ã«å¤±æ•—:`, error);
      return null;
    }
  });

  const deployedExperiments = await Promise.all(deploymentPromises);
  
  logger.info(`ğŸ“ˆ ${deployedExperiments.filter(Boolean).length} å€‹ã®å®Ÿé¨“ãŒç¨¼åƒä¸­ã§ã™`);
}

// å®Ÿè¡Œä¾‹
trainLargeLanguageModel();
runInferenceService();
researchExperiment();
```

## ç›£è¦–ã¨ã‚³ã‚¹ãƒˆæœ€é©åŒ–

**ğŸ“Š ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–ã‚¯ãƒ©ã‚¹**

```typescript:src/services/ResourceMonitor.ts
import { MLPlatformManager } from './MLPlatformManager';
import { logger } from '../utils/logger';

export interface ResourceUsage {
  resourceName: string;
  computeType: 'tpu' | 'gpu';
  zone: string;
  status: string;
  runningTime: number; // minutes
  estimatedCost: number;
  utilizationScore?: number;
}

export interface CostAlert {
  severity: 'info' | 'warning' | 'critical';
  message: string;
  recommendedAction?: string;
  affectedResources: string[];
}

export class ResourceMonitor {
  private mlPlatform: MLPlatformManager;
  private alertThresholds = {
    dailyBudget: 10000, // æ—¥é¡äºˆç®—ï¼ˆå††ï¼‰
    idleTimeMinutes: 60, // ã‚¢ã‚¤ãƒ‰ãƒ«çŠ¶æ…‹ã®é–¾å€¤ï¼ˆåˆ†ï¼‰
    lowUtilizationThreshold: 0.3, // ä½ä½¿ç”¨ç‡ã®é–¾å€¤
  };

  constructor(projectId: string) {
    this.mlPlatform = new MLPlatformManager(projectId);
  }

  async getAllResourceUsage(zones: string[]): Promise<ResourceUsage[]> {
    const allUsage: ResourceUsage[] = [];

    for (const zone of zones) {
      try {
        // TPUãƒªã‚½ãƒ¼ã‚¹ã®å–å¾—
        const tpuNodes = await this.mlPlatform['tpuManager'].listTPUNodes(zone);
        const tpuUsage = tpuNodes.map(node => ({
          resourceName: node.name,
          computeType: 'tpu' as const,
          zone,
          status: node.state,
          runningTime: this.calculateRunningTime(node.createTime),
          estimatedCost: this.estimateCurrentCost(node.acceleratorType, 'tpu', node.createTime),
        }));

        // GPUãƒªã‚½ãƒ¼ã‚¹ã®å–å¾—
        const gpuInstances = await this.mlPlatform['gpuManager'].listGPUInstances(zone);
        const gpuUsage = gpuInstances.map(instance => ({
          resourceName: instance.name,
          computeType: 'gpu' as const,
          zone,
          status: instance.status,
          runningTime: this.calculateRunningTime(instance.createdAt),
          estimatedCost: this.estimateCurrentCost(instance.gpuInfo.type, 'gpu', instance.createdAt),
        }));

        allUsage.push(...tpuUsage, ...gpuUsage);

      } catch (error) {
        logger.error(`Failed to get resource usage for zone ${zone}:`, error);
      }
    }

    return allUsage;
  }

  async generateCostAlerts(usage: ResourceUsage[]): Promise<CostAlert[]> {
    const alerts: CostAlert[] = [];

    // ç·ã‚³ã‚¹ãƒˆã®è¨ˆç®—
    const totalDailyCost = usage.reduce((sum, resource) => {
      if (resource.status === 'RUNNING' || resource.status === 'READY') {
        return sum + resource.estimatedCost;
      }
      return sum;
    }, 0);

    // äºˆç®—è¶…éã‚¢ãƒ©ãƒ¼ãƒˆ
    if (totalDailyCost > this.alertThresholds.dailyBudget) {
      alerts.push({
        severity: 'critical',
        message: `æ—¥æ¬¡äºˆç®—ã‚’è¶…éã—ã¦ã„ã¾ã™: Â¥${totalDailyCost} (äºˆç®—: Â¥${this.alertThresholds.dailyBudget})`,
        recommendedAction: 'ä½¿ç”¨ç‡ã®ä½ã„ãƒªã‚½ãƒ¼ã‚¹ã‚’åœæ­¢ã—ã¦ãã ã•ã„',
        affectedResources: usage.map(r => r.resourceName),
      });
    }

    // é•·æ™‚é–“å®Ÿè¡Œãƒªã‚½ãƒ¼ã‚¹ã®ã‚¢ãƒ©ãƒ¼ãƒˆ
    const longRunningResources = usage.filter(
      resource => resource.runningTime > this.alertThresholds.idleTimeMinutes * 60
    );

    if (longRunningResources.length > 0) {
      alerts.push({
        severity: 'warning',
        message: `${longRunningResources.length}å€‹ã®ãƒªã‚½ãƒ¼ã‚¹ãŒé•·æ™‚é–“å®Ÿè¡Œä¸­ã§ã™`,
        recommendedAction: 'å­¦ç¿’å®Œäº†ã—ãŸãƒªã‚½ãƒ¼ã‚¹ã¯åœæ­¢ã—ã¦ã‚³ã‚¹ãƒˆã‚’å‰Šæ¸›ã—ã¦ãã ã•ã„',
        affectedResources: longRunningResources.map(r => r.resourceName),
      });
    }

    // ã‚¢ã‚¤ãƒ‰ãƒ«çŠ¶æ…‹ã®ãƒªã‚½ãƒ¼ã‚¹æ¤œå‡º
    const idleResources = usage.filter(
      resource => resource.status === 'READY' && 
                  resource.runningTime > this.alertThresholds.idleTimeMinutes
    );

    if (idleResources.length > 0) {
      alerts.push({
        severity: 'warning',
        message: `${idleResources.length}å€‹ã®ãƒªã‚½ãƒ¼ã‚¹ãŒã‚¢ã‚¤ãƒ‰ãƒ«çŠ¶æ…‹ã§ã™`,
        recommendedAction: 'ä½¿ç”¨ã—ã¦ã„ãªã„ãƒªã‚½ãƒ¼ã‚¹ã‚’åœæ­¢ã—ã¦ãã ã•ã„',
        affectedResources: idleResources.map(r => r.resourceName),
      });
    }

    return alerts;
  }

  async executeAutomatedOptimization(alerts: CostAlert[]): Promise<void> {
    const criticalAlerts = alerts.filter(alert => alert.severity === 'critical');
    
    if (criticalAlerts.length === 0) {
      logger.info('ğŸ“Š è‡ªå‹•æœ€é©åŒ–ãŒå¿…è¦ãªã‚¢ãƒ©ãƒ¼ãƒˆã¯ã‚ã‚Šã¾ã›ã‚“');
      return;
    }

    logger.info('ğŸ¤– è‡ªå‹•æœ€é©åŒ–ã‚’å®Ÿè¡Œä¸­...');

    for (const alert of criticalAlerts) {
      // æœ€ã‚‚ã‚³ã‚¹ãƒˆã®é«˜ã„ãƒªã‚½ãƒ¼ã‚¹ã‹ã‚‰é †æ¬¡åœæ­¢
      const resourceUsage = await this.getAllResourceUsage(['us-central1-a', 'asia-northeast1-a']);
      const sortedByCost = resourceUsage
        .filter(r => alert.affectedResources.includes(r.resourceName))
        .sort((a, b) => b.estimatedCost - a.estimatedCost);

      for (const resource of sortedByCost.slice(0, 2)) { // ä¸Šä½2ã¤ã®ãƒªã‚½ãƒ¼ã‚¹ã‚’åœæ­¢
        try {
          if (resource.computeType === 'tpu') {
            await this.mlPlatform['tpuManager'].stopTPUNode(resource.resourceName, resource.zone);
          } else {
            // GPUã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã®å ´åˆã¯å‰Šé™¤ï¼ˆåœæ­¢æ©Ÿèƒ½ãŒãªã„ãŸã‚ï¼‰
            logger.warn(`GPU instance ${resource.resourceName} needs manual attention`);
          }
          
          logger.info(`â¹ï¸ ãƒªã‚½ãƒ¼ã‚¹ ${resource.resourceName} ã‚’è‡ªå‹•åœæ­¢ã—ã¾ã—ãŸ`);
        } catch (error) {
          logger.error(`Failed to auto-optimize resource ${resource.resourceName}:`, error);
        }
      }
    }
  }

  async startMonitoring(zones: string[], intervalMinutes = 30): Promise<void> {
    logger.info(`ğŸ“Š ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–ã‚’é–‹å§‹ã—ã¾ã™ï¼ˆé–“éš”: ${intervalMinutes}åˆ†ï¼‰`);

    const monitoringInterval = setInterval(async () => {
      try {
        const usage = await this.getAllResourceUsage(zones);
        const alerts = await this.generateCostAlerts(usage);

        if (alerts.length > 0) {
          logger.warn(`âš ï¸ ${alerts.length}å€‹ã®ã‚³ã‚¹ãƒˆã‚¢ãƒ©ãƒ¼ãƒˆãŒç™ºç”Ÿã—ã¾ã—ãŸ:`);
          alerts.forEach(alert => {
            logger.warn(`   ${alert.severity.toUpperCase()}: ${alert.message}`);
            if (alert.recommendedAction) {
              logger.warn(`   æ¨å¥¨ã‚¢ã‚¯ã‚·ãƒ§ãƒ³: ${alert.recommendedAction}`);
            }
          });

          // è‡ªå‹•æœ€é©åŒ–ã®å®Ÿè¡Œ
          await this.executeAutomatedOptimization(alerts);
        }

        // ãƒ¬ãƒãƒ¼ãƒˆã®å‡ºåŠ›
        this.generateResourceReport(usage);

      } catch (error) {
        logger.error('ç›£è¦–ã‚µã‚¤ã‚¯ãƒ«ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ:', error);
      }
    }, intervalMinutes * 60 * 1000);

    // Graceful shutdown
    process.on('SIGINT', () => {
      clearInterval(monitoringInterval);
      logger.info('ğŸ“Š ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–ã‚’çµ‚äº†ã—ã¾ã—ãŸ');
      process.exit(0);
    });
  }

  private calculateRunningTime(createTime: string): number {
    const created = new Date(createTime);
    const now = new Date();
    return Math.floor((now.getTime() - created.getTime()) / (1000 * 60)); // minutes
  }

  private estimateCurrentCost(acceleratorType: string, computeType: string, createTime: string): number {
    const runningTimeHours = this.calculateRunningTime(createTime) / 60;
    
    if (computeType === 'tpu') {
      const hourlyRates = {
        'v2-8': 4.5,
        'v3-8': 8.0,
        'v4-8': 16.0,
        'v5p-8': 24.0,
      };
      return Math.round((hourlyRates[acceleratorType] || 8.0) * runningTimeHours);
    } else {
      const hourlyRates = {
        'nvidia-l4': 0.6,
        'nvidia-tesla-a100': 2.9,
        'nvidia-h100-80gb': 8.5,
      };
      return Math.round((hourlyRates[acceleratorType] || 2.0) * runningTimeHours);
    }
  }

  private generateResourceReport(usage: ResourceUsage[]): void {
    const totalCost = usage.reduce((sum, r) => sum + r.estimatedCost, 0);
    const runningCount = usage.filter(r => r.status === 'RUNNING' || r.status === 'READY').length;

    logger.info('ğŸ“‹ ãƒªã‚½ãƒ¼ã‚¹ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ:');
    logger.info(`   ç·ãƒªã‚½ãƒ¼ã‚¹æ•°: ${usage.length}`);
    logger.info(`   ç¨¼åƒä¸­ãƒªã‚½ãƒ¼ã‚¹: ${runningCount}`);
    logger.info(`   æ¨å®šç·ã‚³ã‚¹ãƒˆ: Â¥${totalCost}`);
    logger.info(`   TPUãƒªã‚½ãƒ¼ã‚¹: ${usage.filter(r => r.computeType === 'tpu').length}`);
    logger.info(`   GPUãƒªã‚½ãƒ¼ã‚¹: ${usage.filter(r => r.computeType === 'gpu').length}`);
  }
}
```

## ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ã¨ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°

**ğŸ› ï¸ ãƒ­ã‚°è¨­å®š**

```typescript:src/utils/logger.ts
import winston from 'winston';

const logger = winston.createLogger({
  level: process.env.LOG_LEVEL || 'info',
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.errors({ stack: true }),
    winston.format.colorize(),
    winston.format.printf(({ timestamp, level, message, ...meta }) => {
      return `${timestamp} [${level}] ${message} ${Object.keys(meta).length ? JSON.stringify(meta, null, 2) : ''}`;
    })
  ),
  defaultMeta: { service: 'gcp-ml-platform' },
  transports: [
    new winston.transports.File({ 
      filename: 'logs/error.log', 
      level: 'error' 
    }),
    new winston.transports.File({ 
      filename: 'logs/combined.log' 
    }),
    new winston.transports.Console({
      format: winston.format.combine(
        winston.format.colorize(),
        winston.format.simple()
      )
    })
  ],
});

export { logger };
```

**ğŸ“‹ è¨­å®šç®¡ç†**

```typescript:src/config/index.ts
import dotenv from 'dotenv';

dotenv.config();

export const config = {
  projectId: process.env.GOOGLE_CLOUD_PROJECT || '',
  
  // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚¾ãƒ¼ãƒ³ãƒ»ãƒªãƒ¼ã‚¸ãƒ§ãƒ³è¨­å®š
  defaultTPUZone: process.env.DEFAULT_TPU_ZONE || 'us-central1-a',
  defaultGPUZone: process.env.DEFAULT_GPU_ZONE || 'us-central1-a',
  monitoringZones: process.env.MONITORING_ZONES?.split(',') || ['us-central1-a', 'asia-northeast1-a'],
  
  // ç›£è¦–ãƒ»ã‚¢ãƒ©ãƒ¼ãƒˆè¨­å®š
  monitoring: {
    intervalMinutes: parseInt(process.env.MONITORING_INTERVAL || '30'),
    dailyBudgetLimit: parseInt(process.env.DAILY_BUDGET_LIMIT || '10000'),
    idleTimeThresholdMinutes: parseInt(process.env.IDLE_THRESHOLD_MINUTES || '60'),
    enableAutomatedOptimization: process.env.ENABLE_AUTO_OPTIMIZATION === 'true',
  },
  
  // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®TPUè¨­å®š
  defaultTPUConfig: {
    runtimeVersions: {
      tensorflow: 'tpu-vm-tf-2.15.0-v4',
      pytorch: 'tpu-vm-pt-2.1-v4',
      jax: 'tpu-vm-base',
    },
  },
  
  // ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®GPUè¨­å®š
  defaultGPUConfig: {
    diskImages: {
      tensorflow: 'c0-deeplearning-common-cu121-v20240922-ubuntu-2204',
      pytorch: 'c0-deeplearning-common-cu121-v20240922-ubuntu-2204',
      general: 'c0-deeplearning-common-cu121-v20240922-ubuntu-2204',
    },
    networkTags: ['ml-workload', 'gpu-instance'],
  },
};

// å¿…é ˆè¨­å®šã®æ¤œè¨¼
if (!config.projectId) {
  throw new Error('GOOGLE_CLOUD_PROJECT environment variable is required');
}
```

**ğŸ¯ çµ±åˆCLIãƒ„ãƒ¼ãƒ«**

```typescript:src/cli/ml-platform-cli.ts
#!/usr/bin/env node
import { Command } from 'commander';
import { MLPlatformManager } from '../services/MLPlatformManager';
import { ResourceMonitor } from '../services/ResourceMonitor';
import { config } from '../config';
import { logger } from '../utils/logger';

const program = new Command();
const mlPlatform = new MLPlatformManager(config.projectId);
const monitor = new ResourceMonitor(config.projectId);

program
  .name('ml-platform')
  .description('GCP ML Platform Management CLI')
  .version('1.0.0');

// ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã®æ¨å¥¨ã‚’å–å¾—
program
  .command('recommend')
  .description('Get compute recommendations for ML workload')
  .option('-t, --type <type>', 'workload type (training|inference|research)', 'training')
  .option('-f, --framework <framework>', 'ML framework (tensorflow|pytorch|jax)', 'tensorflow')
  .option('-s, --size <size>', 'model size (small|medium|large|xlarge)', 'medium')
  .option('-z, --zone <zone>', 'GCP zone', config.defaultTPUZone)
  .option('-b, --budget <budget>', 'budget limit (JPY)', '5000')
  .action(async (options) => {
    const workloadConfig = {
      workloadType: options.type as any,
      model: {
        name: `model-${Date.now()}`,
        framework: options.framework as any,
        size: options.size as any,
      },
      compute: {
        preferredType: 'auto' as const,
        zone: options.zone,
        budget: parseInt(options.budget),
      },
    };

    try {
      const recommendation = await mlPlatform.recommendCompute(workloadConfig);
      
      console.log('ğŸ¤– ML Platform æ¨å¥¨çµæœ:');
      console.log(`æ¨å¥¨ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ãƒˆ: ${recommendation.recommendedType.toUpperCase()}`);
      console.log(`æ¨å®šã‚³ã‚¹ãƒˆ: Â¥${recommendation.estimatedCost}`);
      console.log('ç†ç”±:');
      recommendation.reasoning.forEach(reason => console.log(`  â€¢ ${reason}`));
      
    } catch (error) {
      logger.error('æ¨å¥¨ã®å–å¾—ã«å¤±æ•—:', error);
      process.exit(1);
    }
  });

// ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã®ãƒ‡ãƒ—ãƒ­ã‚¤
program
  .command('deploy')
  .description('Deploy ML workload')
  .requiredOption('-n, --name <name>', 'workload name')
  .option('-t, --type <type>', 'compute type (tpu|gpu)', 'auto')
  .option('-z, --zone <zone>', 'GCP zone', config.defaultTPUZone)
  .action(async (options) => {
    // ãƒ‡ãƒ—ãƒ­ã‚¤ãƒ­ã‚¸ãƒƒã‚¯ã®å®Ÿè£…
    console.log(`ğŸš€ ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰ã€Œ${options.name}ã€ã‚’ãƒ‡ãƒ—ãƒ­ã‚¤ä¸­...`);
    // å®Ÿè£…çœç•¥...
  });

// ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–ã®é–‹å§‹
program
  .command('monitor')
  .description('Start resource monitoring')
  .option('-i, --interval <minutes>', 'monitoring interval in minutes', '30')
  .action(async (options) => {
    const intervalMinutes = parseInt(options.interval);
    await monitor.startMonitoring(config.monitoringZones, intervalMinutes);
  });

// ãƒªã‚½ãƒ¼ã‚¹ã®ãƒªã‚¹ãƒˆè¡¨ç¤º
program
  .command('list')
  .description('List all ML resources')
  .option('-z, --zone <zone>', 'specific zone to list', '')
  .action(async (options) => {
    try {
      const zones = options.zone ? [options.zone] : config.monitoringZones;
      const usage = await monitor.getAllResourceUsage(zones);
      
      console.log('ğŸ“‹ ç¨¼åƒä¸­ã®MLãƒªã‚½ãƒ¼ã‚¹:');
      console.table(usage);
      
    } catch (error) {
      logger.error('ãƒªã‚½ãƒ¼ã‚¹ä¸€è¦§ã®å–å¾—ã«å¤±æ•—:', error);
      process.exit(1);
    }
  });

program.parse();
```

## ã¾ã¨ã‚

GCP Cloud TPUãƒ»GPU Ã— TypeScript ã®çµ„ã¿åˆã‚ã›ã§ã€é«˜æ€§èƒ½ãªæ©Ÿæ¢°å­¦ç¿’åŸºç›¤ã®è‡ªå‹•ç®¡ç†ãŒå®Ÿç¾ã§ãã¾ã—ãŸã€‚

**âœ… ã“ã®è¨˜äº‹ã§èº«ã«ã¤ã„ãŸã‚¹ã‚­ãƒ«**

>* Cloud TPU ã¨ GPU ã®ç‰¹å¾´ç†è§£ã¨é©åˆ‡ãªä½¿ã„åˆ†ã‘åˆ¤æ–­
>* TypeScript + Google Cloud Client Library ã§ã® TPUãƒ»GPU è‡ªå‹•åŒ–
>* ãƒ¯ãƒ¼ã‚¯ãƒ­ãƒ¼ãƒ‰æœ€é©åŒ–ã¨æ¨å¥¨ã‚·ã‚¹ãƒ†ãƒ ã®å®Ÿè£…ãƒ‘ã‚¿ãƒ¼ãƒ³  
>* ãƒªã‚½ãƒ¼ã‚¹ç›£è¦–ã¨ã‚³ã‚¹ãƒˆæœ€é©åŒ–ã®è‡ªå‹•åŒ–ã‚·ã‚¹ãƒ†ãƒ æ§‹ç¯‰
>* æ©Ÿæ¢°å­¦ç¿’åŸºç›¤ã‚’ã‚³ãƒ¼ãƒ‰ã§å®Œå…¨åˆ¶å¾¡ã™ã‚‹ Infrastructure as Code

**ğŸ¯ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—**

>* Kubernetes Engineï¼ˆGKEï¼‰ã¨ã®é€£æºã§ã‚³ãƒ³ãƒ†ãƒŠã‚ªãƒ¼ã‚±ã‚¹ãƒˆãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
>* Vertex AI ã¨ã®çµ±åˆã§ MLOps ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³æ§‹ç¯‰
>* BigQueryãƒ»Cloud Storage ã¨ã®é€£æºã§ ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³è‡ªå‹•åŒ–

2025å¹´ã®æœ€æ–°TPU v7ã‚„ NVIDIA GB200 Grace Blackwell ã‚’æ´»ç”¨ã—ã¦ã€ã‚ãªãŸã‚‚æ¬¡ä¸–ä»£ã®æ©Ÿæ¢°å­¦ç¿’åŸºç›¤ã‚’ãƒ—ãƒ­ã‚°ãƒ©ãƒãƒ–ãƒ«ã«æ§‹ç¯‰ã—ã¦ã¿ã¾ã›ã‚“ã‹ï¼Ÿã‚³ã‚¹ãƒˆåŠ¹ç‡ã¨æ€§èƒ½ã‚’ä¸¡ç«‹ã—ãŸ AI é–‹ç™ºç’°å¢ƒã§ã€é©æ–°çš„ãªãƒ¢ãƒ‡ãƒ«é–‹ç™ºã‚’åŠ é€Ÿã•ã›ã¾ã—ã‚‡ã†ï¼